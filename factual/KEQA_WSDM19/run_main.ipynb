{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moha/venv/factual/lib/python3.6/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "import re\n",
    "import augment_process_dataset as apd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    seed = 345\n",
    "    cuda = False\n",
    "    gpu = -1\n",
    "    embed_dim = 250\n",
    "    batch_size = 16\n",
    "    dete_model = 'dete_best_model.pt'\n",
    "    entity_model = 'entity_best_model.pt'\n",
    "    pred_model='pred_best_model.pt'\n",
    "    output='preprocess'\n",
    "    data = './data/penn'\n",
    "    model = 'LSTM'\n",
    "    emsize = 200\n",
    "    nhid = 200\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from itertools import compress\n",
    "from evaluation import evaluation, get_span\n",
    "from argparse import ArgumentParser\n",
    "from torchtext import data\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from fuzzywuzzy import fuzz\n",
    "from util import www2fb, processed_text, clean_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dete_model = os.path.join(args.output, args.dete_model)\n",
    "args.entity_model = os.path.join(args.output, args.entity_model)\n",
    "args.pred_model = os.path.join(args.output, args.pred_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_predict(dataset_iter):\n",
    "    model.eval()\n",
    "    dataset_iter.init_epoch()\n",
    "    gold_list = []\n",
    "    pred_list = []\n",
    "    dete_result = []\n",
    "    question_list = []\n",
    "    for data_batch_idx, data_batch in enumerate(dataset_iter):\n",
    "        #batch_size = data_batch.text.size()[1]\n",
    "        answer = torch.max(model(data_batch), 1)[1].view(data_batch.ed.size())\n",
    "        answer[(data_batch.text.data == 1)] = 1\n",
    "        answer = np.transpose(answer.cpu().data.numpy())\n",
    "        gold_list.append(np.transpose(data_batch.ed.cpu().data.numpy()))\n",
    "        index_question = np.transpose(data_batch.text.cpu().data.numpy())\n",
    "        question_array = index2word[index_question]\n",
    "        dete_result.extend(answer)\n",
    "        question_list.extend(question_array)\n",
    "        #for i in range(batch_size):  # If no word is detected as entity, select top 3 possible words\n",
    "        #    if all([j == 1 or j == idxO for j in answer[i]]):\n",
    "        #        index = list(range(i, scores.shape[0], batch_size))\n",
    "        #        FindOidx = [j for j, x in enumerate(answer[i]) if x == idxO]\n",
    "        #        idx_in_socres = [index[j] for j in FindOidx]\n",
    "        #        subscores = scores[idx_in_socres]\n",
    "        #        answer[i][torch.sort(torch.max(subscores, 1)[0], descending=True)[1][0:min(2, len(FindOidx))]] = idxI\n",
    "        pred_list.append(answer)\n",
    "    return dete_result, question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: You have Cuda but not use it. You are using CPU for testing.\n"
     ]
    }
   ],
   "source": [
    "def compute_reach_dic(matched_mid):\n",
    "    reach_dic = {}  # reach_dic[head_id] = (pred_id, tail_id)\n",
    "    with open(os.path.join(args.output, 'transE_train.txt'), 'r') as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split(\"\\t\")\n",
    "            head_id = items[0]\n",
    "            if head_id in matched_mid and items[2] in pre_dic:\n",
    "                if reach_dic.get(head_id) is None:\n",
    "                    reach_dic[head_id] = [pre_dic[items[2]]]\n",
    "                else:\n",
    "                    reach_dic[head_id].append(pre_dic[items[2]])\n",
    "    return reach_dic\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "if not args.cuda:\n",
    "    args.gpu = -1\n",
    "if torch.cuda.is_available() and args.cuda:\n",
    "    print(\"Note: You are using GPU for testing\")\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "if torch.cuda.is_available() and not args.cuda:\n",
    "    print(\"Warning: You have Cuda but not use it. You are using CPU for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a question: where is italy\n"
     ]
    }
   ],
   "source": [
    "############################## User Input ###########################\n",
    "TEXT = data.Field(lower=True)\n",
    "ED = data.Field()\n",
    "ip = input('enter a question: ')\n",
    "ip = processed_text(ip)\n",
    "#m.0h5t1m8\twho produced the film woodstock villa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(os.path.join(args.output, 'input_file.txt'), 'a')\n",
    "tok = apd.reverseLinking(ip,None)\n",
    "tok = tok[1]\n",
    "outfile.write('{}\\t{}\\t\\n'.format(ip, tok))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= data.TabularDataset(path=os.path.join(args.output, 'input_file.txt'), format='tsv', fields=[('text', TEXT), ('ed', ED)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.TabularDataset(path=os.path.join(args.output, 'dete_train.txt'), format='tsv', fields=[('text', TEXT), ('ed', ED)])\n",
    "field = [('id', None), ('sub', None), ('entity', None), ('relation', None), ('obj', None), ('text', TEXT), ('ed', ED)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of example: 21688\n"
     ]
    }
   ],
   "source": [
    "dev, test = data.TabularDataset.splits(path=args.output, validation='valid.txt', test='test.txt', format='tsv', fields=field)\n",
    "TEXT.build_vocab(train, dev, test)\n",
    "ED.build_vocab(train, dev)\n",
    "total_num = len(test)\n",
    "print('total num of example: {}'.format(total_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "if args.gpu == -1: # Load all tensors onto the CPU\n",
    "    test_iter = data.Iterator(out, batch_size=args.batch_size, train=False, repeat=False, sort=False, shuffle=False, \n",
    "                              sort_within_batch=False)\n",
    "    model = torch.load(args.dete_model, map_location=lambda storage, loc: storage)\n",
    "    model.config.cuda = False\n",
    "else:\n",
    "    test_iter = data.Iterator(test, batch_size=args.batch_size, device=torch.device('cuda', args.gpu), train=False,\n",
    "                              repeat=False, sort=False, shuffle=False, sort_within_batch=False)\n",
    "    model = torch.load(args.dete_model, map_location=lambda storage, loc: storage.cuda(args.gpu))\n",
    "index2tag = np.array(ED.vocab.itos)\n",
    "idxO = int(np.where(index2tag == 'O')[0][0])  # Index for 'O'\n",
    "idxI = int(np.where(index2tag == 'I')[0][0])  # Index for 'I'\n",
    "index2word = np.array(TEXT.vocab.itos)\n",
    "# run the model on the test set and write the output to a file\n",
    "dete_result, question_list= entity_predict(dataset_iter=test_iter)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "realed=[]\n",
    "for i in range(len(dete_result[0])):\n",
    "    if dete_result[0][i] == 2:\n",
    "        realed.append('O')\n",
    "    if dete_result[0][i] == 3:\n",
    "        realed.append('I')\n",
    "    realed.append(' ')\n",
    "del realed[-1]\n",
    "realed=''.join(realed)\n",
    "\n",
    "emp = ''\n",
    "outfile = open(os.path.join(args.output, 'test.txt'), 'a')\n",
    "outfile.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t\\n'.format(emp,emp,emp,emp,emp,ip, realed))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Entity Detection  ########################\n",
    "TEXT = data.Field(lower=True)\n",
    "ED = data.Field()\n",
    "train = data.TabularDataset(path=os.path.join(args.output, 'dete_train.txt'), format='tsv', fields=[('text', TEXT), ('ed', ED)])\n",
    "field = [('id', None), ('sub', None), ('entity', None), ('relation', None), ('obj', None), ('text', TEXT), ('ed', ED)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= data.TabularDataset(path=os.path.join(args.output, 'input_file.txt'), format='tsv', fields=[('text', TEXT), ('ed', ED)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21689\n"
     ]
    }
   ],
   "source": [
    "dev, test = data.TabularDataset.splits(path=args.output, validation='valid.txt', test='test.txt', format='tsv', fields=field)\n",
    "TEXT.build_vocab(train, dev, test)\n",
    "ED.build_vocab(train, dev)\n",
    "total_num = len(test)\n",
    "print(total_num)\n",
    "# load the model\n",
    "if args.gpu == -1: # Load all tensors onto the CPU\n",
    "    test_iter = data.Iterator(test, batch_size=args.batch_size, train=False, repeat=False, sort=False, shuffle=False, \n",
    "                              sort_within_batch=False)\n",
    "    model = torch.load(args.dete_model, map_location=lambda storage, loc: storage)\n",
    "    model.config.cuda = False\n",
    "else:\n",
    "    test_iter = data.Iterator(test, batch_size=args.batch_size, device=torch.device('cuda', args.gpu), train=False,\n",
    "                              repeat=False, sort=False, shuffle=False, sort_within_batch=False)\n",
    "    model = torch.load(args.dete_model, map_location=lambda storage, loc: storage.cuda(args.gpu))\n",
    "index2tag = np.array(ED.vocab.itos)\n",
    "idxO = int(np.where(index2tag == 'O')[0][0])  # Index for 'O'\n",
    "idxI = int(np.where(index2tag == 'I')[0][0])  # Index for 'I'\n",
    "index2word = np.array(TEXT.vocab.itos)\n",
    "# run the model on the test set and write the output to a file\n",
    "dete_result, question_list = entity_predict(dataset_iter=test_iter)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Find matched names  ########################\n",
    "mid_dic, mid_num_dic = {}, {}  # Dictionary for MID\n",
    "for line in open(os.path.join(args.output, 'entity2id.txt'), 'r'):\n",
    "    items = line.strip().split(\"\\t\")\n",
    "    mid_dic[items[0]] = int(items[1])\n",
    "    mid_num_dic[int(items[1])] = items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_dic, pre_num_dic = {}, {}  # Dictionary for predicates\n",
    "match_pool = []\n",
    "for line in open(os.path.join(args.output, 'relation2id.txt'), 'r'):\n",
    "    items = line.strip().split(\"\\t\")\n",
    "    match_pool = match_pool + items[0].replace('.', ' ').replace('_', ' ').split()\n",
    "    pre_dic[items[0]] = int(items[1])\n",
    "    pre_num_dic[int(items[1])] = items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_emb = np.fromfile(os.path.join(args.output, 'entities_emb.bin'), dtype=np.float32).reshape((len(mid_dic), args.embed_dim))\n",
    "predicates_emb = np.fromfile(os.path.join(args.output, 'predicates_emb.bin'), dtype=np.float32).reshape((-1, args.embed_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#names_map = {}\n",
    "index_names = {}\n",
    "\n",
    "for i, line in enumerate(open(os.path.join(args.output, 'names.trimmed.txt'), 'r')):\n",
    "    items = line.strip().split(\"\\t\")\n",
    "    entity = items[0]\n",
    "    literal = items[1].strip()\n",
    "    if literal != \"\":\n",
    "        #if names_map.get(entity) is None or len(names_map[entity].split()) > len(literal.split()):\n",
    "        #    names_map[entity] = literal\n",
    "        if index_names.get(literal) is None:\n",
    "            index_names[literal] = [entity]\n",
    "        else:\n",
    "            index_names[literal].append(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in [\"train.txt\", \"valid.txt\"]:\n",
    "    with open(os.path.join(args.output, fname), 'r') as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split(\"\\t\")\n",
    "            if items[2] != '<UNK>' and mid_dic.get(items[1]) is not None:\n",
    "                if index_names.get(items[2]) is None:\n",
    "                    index_names[items[2]] = [items[1]]\n",
    "                else:\n",
    "                    index_names[items[2]].append(items[1])\n",
    "                #if names_map.get(items[1]) is None or len(names_map[items[1]].split()) > len(items[2].split()):\n",
    "                #    names_map[items[1]] = items[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/moha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#for fname in [\"train.txt\", \"valid.txt\"]:\n",
    "#    with open(os.path.join(args.output, fname), 'r') as f:\n",
    "#        for line in f:\n",
    "#            items = line.strip().split(\"\\t\")\n",
    "#            match_pool.extend(list(compress(items[5].split(), [element == 'O' for element in items[6].split()])))\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "head_mid_idx = [[] for i in range(total_num)]# [[head1,head2,...], [head1,head2,...], ...]\n",
    "match_pool = set(match_pool + stopwords.words('english') + [\"'s\"])\n",
    "whhowset = [{'what', 'how', 'where', 'who', 'which', 'whom'},\n",
    "            {'in which', 'what is', \"what 's\", 'what are', 'what was', 'what were', 'where is', 'where are',\n",
    "             'where was', 'where were', 'who is', 'who was', 'who are', 'how is', 'what did'},\n",
    "            {'what kind of', 'what kinds of', 'what type of', 'what types of', 'what sort of'}]\n",
    "dete_tokens_list, filter_q = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, question in enumerate(question_list):\n",
    "    question = [token for token in question if token != '<pad>']\n",
    "    pred_span = get_span(dete_result[i], index2tag, type=False)\n",
    "    tokens_list, dete_tokens, st, en, changed = [], [], 0, 0, 0\n",
    "    for st, en in pred_span:\n",
    "        tokens = question[st:en]\n",
    "        tokens_list.append(tokens)\n",
    "        if index_names.get(' '.join(tokens)) is not None:  # important\n",
    "            dete_tokens.append(' '.join(tokens))\n",
    "            head_mid_idx[i].append(' '.join(tokens))\n",
    "    if len(question) > 2:\n",
    "        for j in range(3, 0, -1):\n",
    "            if ' '.join(question[0:j]) in whhowset[j - 1]:\n",
    "                changed = j\n",
    "                del question[0:j]\n",
    "                continue\n",
    "    tokens_list.append(question)\n",
    "    filter_q.append(' '.join(question[:st - changed] + question[en - changed:]))\n",
    "    if not head_mid_idx[i]:\n",
    "        dete_tokens = question\n",
    "        for tokens in tokens_list:\n",
    "            grams = []\n",
    "            maxlen = len(tokens)\n",
    "            for j in range(maxlen - 1, 1, -1):\n",
    "                for token in [tokens[idx:idx + j] for idx in range(maxlen - j + 1)]:\n",
    "                    grams.append(' '.join(token))\n",
    "            for gram in grams:\n",
    "                if index_names.get(gram) is not None:\n",
    "                    head_mid_idx[i].append(gram)\n",
    "                    break\n",
    "            for j, token in enumerate(tokens):\n",
    "                if token not in match_pool:\n",
    "                    tokens = tokens[j:]\n",
    "                    break\n",
    "            if index_names.get(' '.join(tokens)) is not None:\n",
    "                head_mid_idx[i].append(' '.join(tokens))\n",
    "            tokens = tokens[::-1]\n",
    "            for j, token in enumerate(tokens):\n",
    "                if token not in match_pool:\n",
    "                    tokens = tokens[j:]\n",
    "                    break\n",
    "            tokens = tokens[::-1]\n",
    "            if index_names.get(' '.join(tokens)) is not None:\n",
    "                head_mid_idx[i].append(' '.join(tokens))\n",
    "    dete_tokens_list.append(' '.join(dete_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_match = set()\n",
    "match_mid_list = []\n",
    "tupleset = []\n",
    "for i, names in enumerate(head_mid_idx):\n",
    "    tuplelist = []\n",
    "    for name in names:\n",
    "        mids = index_names[name]\n",
    "        match_mid_list.extend(mids)\n",
    "        for mid in mids:\n",
    "            if mid_dic.get(mid) is not None:\n",
    "                tuplelist.append((mid, name))\n",
    "    tupleset.extend(tuplelist)\n",
    "    head_mid_idx[i] = list(set(tuplelist))\n",
    "    if tuplelist:\n",
    "        id_match.add(i)\n",
    "tupleset = set(tupleset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tuple_topic = []\n",
    "with open('/home/keith/Documents/KEQA_WSDM_Download/data/FB5M.name.txt', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i % 1000000 == 0:\n",
    "            print(\"line: {}\".format(i))\n",
    "        items = line.strip().split(\"\\t\")\n",
    "        if (www2fb(clean_uri(items[0])), processed_text(clean_uri(items[2]))) in tupleset and items[1] == \"<fb:type.object.name>\":\n",
    "            tuple_topic.append((www2fb(clean_uri(items[0])), processed_text(clean_uri(items[2]))))\n",
    "tuple_topic = set(tuple_topic)\n",
    "\n",
    "with open('tuple_topic.txt', 'wb') as fp:\n",
    "    pickle.dump(tuple_topic, fp)'''\n",
    "import pickle\n",
    "with open ('tuple_topic.txt', 'rb') as fp:\n",
    "    tuple_topic = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Learn entity representation  ########################\n",
    "head_emb = np.zeros((total_num, args.embed_dim))\n",
    "TEXT = data.Field(lower=True)\n",
    "ED = data.Field(sequential=False, use_vocab=False)\n",
    "train, dev = data.TabularDataset.splits(path=args.output, train='entity_train.txt', validation='entity_valid.txt', format='tsv', fields=[('text', TEXT), ('mid', ED)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = [('id', None), ('sub', None), ('entity', None), ('relation', None), ('obj', None), ('text', TEXT), ('ed', None)]\n",
    "test = data.TabularDataset(path=os.path.join(args.output,'test.txt'), format='tsv', fields=field)\n",
    "TEXT.build_vocab(train, dev, test)  # training data includes validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "if args.gpu == -1:  # Load all tensors onto the CPU\n",
    "    test_iter = data.Iterator(test, batch_size=args.batch_size, train=False, repeat=False, sort=False, shuffle=False, \n",
    "                              sort_within_batch=False)\n",
    "    model = torch.load(args.entity_model, map_location=lambda storage, loc: storage)\n",
    "    model.config.cuda = False\n",
    "else:\n",
    "    test_iter = data.Iterator(test, batch_size=args.batch_size, device=torch.device('cuda', args.gpu), train=False,\n",
    "                              repeat=False, sort=False, shuffle=False, sort_within_batch=False)\n",
    "    model = torch.load(args.entity_model, map_location=lambda storage, loc: storage.cuda(args.gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_iter.init_epoch()\n",
    "baseidx = 0\n",
    "for data_batch_idx, data_batch in enumerate(test_iter):\n",
    "    batch_size = data_batch.text.size()[1]\n",
    "    scores = model(data_batch).cpu().data.numpy()\n",
    "    for i in range(batch_size):\n",
    "        head_emb[baseidx + i] = scores[i]\n",
    "    baseidx = baseidx + batch_size\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Learn predicate representation  ########################\n",
    "TEXT = data.Field(lower=True)\n",
    "ED = data.Field(sequential=False, use_vocab=False)\n",
    "train, dev = data.TabularDataset.splits(path=args.output, train='pred_train.txt', validation='pred_valid.txt', format='tsv', fields=[('text', TEXT), ('mid', ED)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = [('id', None), ('sub', None), ('entity', None), ('relation', None), ('obj', None), ('text', TEXT), ('ed', None)]\n",
    "test = data.TabularDataset(path=os.path.join(args.output,'test.txt'), format='tsv', fields=field)\n",
    "TEXT.build_vocab(train, dev, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "if args.gpu == -1:  # Load all tensors onto the CPU\n",
    "    test_iter = data.Iterator(test, batch_size=args.batch_size, train=False, repeat=False, sort=False, shuffle=False, \n",
    "                              sort_within_batch=False)\n",
    "    model = torch.load(args.pred_model, map_location=lambda storage, loc: storage)\n",
    "    model.config.cuda = False\n",
    "else:\n",
    "    test_iter = data.Iterator(test, batch_size=args.batch_size, device=torch.device('cuda', args.gpu), train=False,\n",
    "                              repeat=False, sort=False, shuffle=False, sort_within_batch=False)\n",
    "    model = torch.load(args.pred_model, map_location=lambda storage, loc: storage.cuda(args.gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_iter.init_epoch()\n",
    "baseidx = 0\n",
    "pred_emb = np.zeros((total_num, args.embed_dim))\n",
    "for data_batch_idx, data_batch in enumerate(test_iter):\n",
    "    batch_size = data_batch.text.size()[1]\n",
    "    scores = model(data_batch).cpu().data.numpy()\n",
    "    for i in range(batch_size):\n",
    "        s = scores[i]\n",
    "        pred_emb[baseidx + i] = s        \n",
    "    baseidx = baseidx + batch_size\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gt_tail = []  #  Ground Truth\\ngt_pred = []\\ngt_head = []  # Ground Truth of head entity\\nfor line in open(os.path.join(args.output,\\'test.txt\\'), \\'r\\'):\\n    items = line.strip().split(\"\\t\")\\n    gt_head.append(items[1])\\n    gt_pred.append(items[3])\\n    gt_tail.append(items[4])'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"gt_tail = []  #  Ground Truth\n",
    "gt_pred = []\n",
    "gt_head = []  # Ground Truth of head entity\n",
    "for line in open(os.path.join(args.output,'test.txt'), 'r'):\n",
    "    items = line.strip().split(\"\\t\")\n",
    "    gt_head.append(items[1])\n",
    "    gt_pred.append(items[3])\n",
    "    gt_tail.append(items[4])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "notmatch = list(set(range(0, total_num)).symmetric_difference(id_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "notmatch_idx = euclidean_distances(head_emb[notmatch], entities_emb, squared=True).argsort(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, i in enumerate(notmatch):\n",
    "    for j in notmatch_idx[idx, 0:40]:\n",
    "        mid = mid_num_dic[j]\n",
    "        head_mid_idx[i].append((mid, None))\n",
    "        match_mid_list.append(mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, mid_num = 0, 0\n",
    "for i, head_ids in enumerate(head_mid_idx):\n",
    "    mids = set()\n",
    "    for (head_id, name) in head_ids:\n",
    "        mids.add(head_id)\n",
    "#    if gt_head[i] in mids:\n",
    "#        correct += 1\n",
    "    mid_num += len(mids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reach_dic = compute_reach_dic(set(match_mid_list))\n",
    "learned_pred, learned_fact, learned_head = [-1] * total_num, {}, [-1] * total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha1, alpha3 = .39, .43\n",
    "for i, head_ids in enumerate(head_mid_idx[-1]):  # head_ids is mids\n",
    "    i = total_num - 1\n",
    "    answers = []\n",
    "    head_id = head_ids[0]\n",
    "    name = head_ids[1]\n",
    "    mid_score = np.sqrt(np.sum(np.power(entities_emb[mid_dic[head_id]] - head_emb[i], 2)))\n",
    "    name_score = - .003 * fuzz.ratio(name, dete_tokens_list[i])\n",
    "    if (head_id, name) in tuple_topic:\n",
    "        name_score -= .18\n",
    "    if reach_dic.get(head_id) is not None:\n",
    "        for pred_id in reach_dic[head_id]:  # reach_dic[head_id] = pred_id are numbers\n",
    "                rel_names = - .017 * fuzz.ratio(pre_num_dic[pred_id].replace('.', ' ').replace('_', ' '), filter_q[i]) #0.017\n",
    "                rel_score = np.sqrt(np.sum(np.power(predicates_emb[pred_id] - pred_emb[i], 2))) + rel_names\n",
    "                tai_score = np.sqrt(np.sum(\n",
    "                    np.power(predicates_emb[pred_id] + entities_emb[mid_dic[head_id]] - head_emb[i] - pred_emb[i], 2)))\n",
    "                answers.append((head_id, pred_id, alpha1 * mid_score + rel_score + alpha3 * tai_score + name_score))\n",
    "    if answers:\n",
    "        answers.sort(key=lambda x: x[2])\n",
    "        learned_head[i] = answers[0][0]\n",
    "        learned_pred[i] = answers[0][1]\n",
    "        learned_fact[' '.join([learned_head[i], pre_num_dic[learned_pred[i]]])] = i\n",
    "learned_tail = [[] for i in range(total_num)]\n",
    "for line in open(os.path.join(args.output, 'cleanedFB.txt'), 'r'):\n",
    "    items = line.strip().split(\"\\t\")\n",
    "    if learned_fact.get(' '.join([items[0], items[2]])) is not None:\n",
    "        learned_tail[learned_fact[' '.join([items[0], items[2]])]].extend(items[1].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "is_empty = 0\n",
    "if len(learned_tail[-1]) == 0:\n",
    "    matches.append(\" Sorry bro, idk\")\n",
    "    is_empty = 1\n",
    "else:\n",
    "    stringToMatch = learned_tail[-1][0]\n",
    "    matchedLine = ''\n",
    "    with open('preprocess/heads_toes.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            potmatch = line.strip().split('\\t')\n",
    "            if stringToMatch == potmatch[0]:\n",
    "                #matchedLine = line.strip().split('\\t')\n",
    "                matches.append(potmatch[1])\n",
    "                is_empty = 1\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "readFile = open(\"preprocess/test.txt\")\n",
    "lines = readFile.readlines()\n",
    "readFile.close()\n",
    "w = open(\"preprocess/test.txt\",'w')\n",
    "w.writelines([item for item in lines[:-1]])\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I dont know bro\n"
     ]
    }
   ],
   "source": [
    "if is_empty == 0:\n",
    "    print('I dont know bro')\n",
    "else:\n",
    "    print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
