{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import re\n",
    "import augment_process_dataset as apd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "#import entity_detection as ed\n",
    "#from test_main import entity_predict\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import compress\n",
    "from evaluation import evaluation, get_span\n",
    "from argparse import ArgumentParser\n",
    "from torchtext import data\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from fuzzywuzzy import fuzz\n",
    "from util import www2fb, processed_text, clean_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    seed = 345\n",
    "    cuda = False\n",
    "    gpu = -1\n",
    "    embed_dim = 250\n",
    "    batch_size = 1\n",
    "    dete_model = 'dete_best_model.pt'\n",
    "    entity_model = 'entity_best_model.pt'\n",
    "    pred_model='pred_best_model.pt'\n",
    "    output='preprocess'\n",
    "    data = './data/penn'\n",
    "    model = 'LSTM'\n",
    "    emsize = 200\n",
    "    nhid = 200\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a question: who is daft punk?\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(lower=True)\n",
    "ED = data.Field()\n",
    "ip = input('enter a question: ')\n",
    "ip = processed_text(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(os.path.join(args.output, 'input_file.txt'), 'w')\n",
    "#with open(os.path.join(args.output, 'freebase-FB5M.txt'), \"r\") as f:\n",
    "#    searchlines = f.readlines()\n",
    "#for i, line in enumerate(searchlines):\n",
    "#    if \"india\" in line: \n",
    "#        for l in searchlines[i:i+3]:\n",
    "#            print (l)\n",
    "ed=apd.reverseLinking(ip,None)\n",
    "ed = ed[1]\n",
    "outfile.write('{}\\t{}\\t\\n'.format(ip, ed))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out= data.TabularDataset(path=os.path.join(args.output, 'input_file.txt'), format='tsv', fields=[('text', TEXT), ('ed', ED)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_predict(dataset_iter):\n",
    "    model.eval()\n",
    "    dataset_iter.init_epoch()\n",
    "    gold_list = []\n",
    "    pred_list = []\n",
    "    dete_result = []\n",
    "    question_list = []\n",
    "    for data_batch_idx, data_batch in enumerate(dataset_iter):\n",
    "        batch_size = 1#data_batch.text.size()[1]\n",
    "        answer = torch.max(model(data_batch), 1)[1].view(data_batch.ed.size())\n",
    "        answer[(data_batch.text.data == 1)] = 1\n",
    "        answer = np.transpose(answer.cpu().data.numpy())\n",
    "        gold_list.append(np.transpose(data_batch.ed.cpu().data.numpy()))\n",
    "        index_question = np.transpose(data_batch.text.cpu().data.numpy())\n",
    "        question_array = index2word[index_question]\n",
    "        dete_result.extend(answer)\n",
    "        question_list.extend(question_array)\n",
    "        #for i in range(batch_size):  # If no word is detected as entity, select top 3 possible words\n",
    "        #    if all([j == 1 or j == idxO for j in answer[i]]):\n",
    "        #        index = list(range(i, scores.shape[0], batch_size))\n",
    "        #        FindOidx = [j for j, x in enumerate(answer[i]) if x == idxO]\n",
    "        #        idx_in_socres = [index[j] for j in FindOidx]\n",
    "        #        subscores = scores[idx_in_socres]\n",
    "        #        answer[i][torch.sort(torch.max(subscores, 1)[0], descending=True)[1][0:min(2, len(FindOidx))]] = idxI\n",
    "        pred_list.append(answer)\n",
    "    return dete_result, question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.TabularDataset(path=os.path.join(args.output, 'dete_train.txt'), format='tsv', fields=[('text', TEXT), ('ed', ED)])\n",
    "field = [('id', None), ('sub', None), ('entity', None), ('relation', None), ('obj', None), ('text', TEXT), ('ed', ED)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num of example: 21687\n"
     ]
    }
   ],
   "source": [
    "dev, test = data.TabularDataset.splits(path=args.output, validation='valid.txt', test='test.txt', format='tsv', fields=field)\n",
    "TEXT.build_vocab(train, dev, test)\n",
    "ED.build_vocab(train, dev)\n",
    "total_num = len(test)\n",
    "print('total num of example: {}'.format(total_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['who', 'is', '<unk>', 'punk', '?'], dtype='<U62')]\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "if args.gpu == -1: # Load all tensors onto the CPU\n",
    "    test_iter = data.Iterator(out, batch_size=args.batch_size, train=False, repeat=False, sort=False, shuffle=False, \n",
    "                              sort_within_batch=False)\n",
    "    model = torch.load(args.dete_model, map_location=lambda storage, loc: storage)\n",
    "    model.config.cuda = False\n",
    "else:\n",
    "    test_iter = data.Iterator(test, batch_size=args.batch_size, device=torch.device('cuda', args.gpu), train=False,\n",
    "                              repeat=False, sort=False, shuffle=False, sort_within_batch=False)\n",
    "    model = torch.load(args.dete_model, map_location=lambda storage, loc: storage.cuda(args.gpu))\n",
    "index2tag = np.array(ED.vocab.itos)\n",
    "idxO = int(np.where(index2tag == 'O')[0][0])  # Index for 'O'\n",
    "idxI = int(np.where(index2tag == 'I')[0][0])  # Index for 'I'\n",
    "index2word = np.array(TEXT.vocab.itos)\n",
    "# run the model on the test set and write the output to a file\n",
    "dete_result, question_list= entity_predict(dataset_iter=test_iter)\n",
    "print(question_list)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "realed=[]\n",
    "for i in range(len(dete_result[0])):\n",
    "    if dete_result[0][i] == 2:\n",
    "        realed.append('O')\n",
    "    if dete_result[0][i] == 3:\n",
    "        realed.append('I')\n",
    "    realed.append(' ')\n",
    "del realed[-1]\n",
    "realed=''.join(realed)  \n",
    "outfile = open(os.path.join(args.output, 'input_file.txt'), 'w')\n",
    "outfile.write('{}\\t{}\\t\\n'.format(ip, realed))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"w_filenameTSV = './preprocess/mid2name.tsv'\\ntsv_read = pd.read_csv(w_filenameTSV, sep='\\t')\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''w_filenameTSV = './preprocess/mid2name.tsv'\n",
    "tsv_read = pd.read_csv(w_filenameTSV, sep='\\t')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"outfile = open(os.path.join(args.output, 'heads_toes.txt'), 'w')\\nmid_cid = []\\nfor i in range(len(tsv_read.mid)):\\n    mid_cid.append(www2fb(tsv_read.mid[i]))\\n    mid_cid[i] = mid_cid[i].replace('m0','m.0')\\n    outfile.write('{}\\t{}\\t\\n'.format(mid_cid[i],tsv_read.name[i]))\\noutfile.close()\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''outfile = open(os.path.join(args.output, 'heads_toes.txt'), 'w')\n",
    "mid_cid = []\n",
    "for i in range(len(tsv_read.mid)):\n",
    "    mid_cid.append(www2fb(tsv_read.mid[i]))\n",
    "    mid_cid[i] = mid_cid[i].replace('m0','m.0')\n",
    "    outfile.write('{}\\t{}\\t\\n'.format(mid_cid[i],tsv_read.name[i]))\n",
    "outfile.close()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f = open(\"./preprocess/heads_toes.txt\", \"r\")\\nprint(f.read(100))'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''f = open(\"./preprocess/heads_toes.txt\", \"r\")\n",
    "print(f.read(100))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from rdflib import Graph\\ng = Graph()\\ng.parse(\"demo.nt\", format=\"nt\")\\n\\nlen(g) # prints 2\\n\\nimport pprint\\nfor stmt in g:\\n    pprint.pprint(stmt)\\n\\n# prints :\\n(rdflib.term.URIRef(\\'http://bigasterisk.com/foaf.rdf#drewp\\'),\\n rdflib.term.URIRef(\\'http://example.com/says\\'),\\n rdflib.term.Literal(u\\'Hello world\\'))\\n(rdflib.term.URIRef(\\'http://bigasterisk.com/foaf.rdf#drewp\\'),\\n rdflib.term.URIRef(\\'http://www.w3.org/1999/02/22-rdf-syntax-ns#type\\'),\\n rdflib.term.URIRef(\\'http://xmlns.com/foaf/0.1/Person\\'))\\n \\n \\n \\n from wikidata.client import Client\\n>>> client = Client()  # doctest: +SKIP\\n>>> entity = client.get(\\'Q20145\\', load=True)\\n>>> entity\\n '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from rdflib import Graph\n",
    "g = Graph()\n",
    "g.parse(\"demo.nt\", format=\"nt\")\n",
    "\n",
    "len(g) # prints 2\n",
    "\n",
    "import pprint\n",
    "for stmt in g:\n",
    "    pprint.pprint(stmt)\n",
    "\n",
    "# prints :\n",
    "(rdflib.term.URIRef('http://bigasterisk.com/foaf.rdf#drewp'),\n",
    " rdflib.term.URIRef('http://example.com/says'),\n",
    " rdflib.term.Literal(u'Hello world'))\n",
    "(rdflib.term.URIRef('http://bigasterisk.com/foaf.rdf#drewp'),\n",
    " rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
    " rdflib.term.URIRef('http://xmlns.com/foaf/0.1/Person'))\n",
    " \n",
    " \n",
    " \n",
    " from wikidata.client import Client\n",
    ">>> client = Client()  # doctest: +SKIP\n",
    ">>> entity = client.get('Q20145', load=True)\n",
    ">>> entity\n",
    " '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
